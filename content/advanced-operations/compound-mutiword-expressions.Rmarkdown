---
title: Compound multi-word expressions
weight: 20
draft: false
---

```{r message=FALSE}
require(quanteda)
require(quanteda.corpora)
```

This corpus contains 6,000 Guardian news articles from 2012 to 2016.

```{r eval=FALSE}
corp_news <- download('data_corpus_guardian')
```

```{r include=FALSE}
# This code is only for website generation
corp_news <- readRDS("../data/data_corpus_guardian.rds")
```

```{r}
ndoc(corp_news)
range(docvars(corp_news, 'date'))
```

Unlike in earlier examples, we remove punctuations in `tokens_remove()` with `padding = TRUE` to keep the original positions of tokens. `[\\p{P}\\p{S}]` is a regular expression for punctuations or separators.

```{r}
toks_news <- tokens(corp_news) %>% 
    tokens_remove(stopwords('english'), padding = TRUE) %>% 
    tokens_remove('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE)
```

## Collocation analysis

Through collocation analysis, we can identify multi-word expressions that are very frequent in newspaper articles. One of the most common type of multi-word expressions is proper names, which we can select simply based on capitalization in English texts.

```{r}
toks_news_cap <- tokens_select(toks_news, 
                               pattern = '^[A-Z]',
                               valuetype = 'regex',
                               case_insensitive = FALSE, 
                               padding = TRUE)
head(toks_news_cap[[1]], 50)

tstat_col_cap <- textstat_collocations(toks_news_cap, min_count = 10, tolower = FALSE)
head(tstat_col_cap, 20)
```

### Compound multi-word expressions

The result of collocation analysis is not only very interesting but useful: you can be use it to compound tokens. Compounding makes tokens less ambiguous and significantly improves quality of statistical analysis in the downstream. We will only compound strongly associated (p<0.005) multi-word expressions here by sub-setting `tstat_col_cap$collocation`.

{{% notice note %}}
Collocations are automatically recognized as multi-word expressions by `tokens_compound()` in *case-sensitive fixed pattern matching*. This is the fastest way to compound large numbers of multi-word expressions, but make sure that `tolower = FALSE` in `textstat_collocations()` to do this.
{{% /notice %}}

```{r}
toks_comp <- tokens_compound(toks_news, pattern = tstat_col_cap[tstat_col_cap$z > 3])
toks_news[['text7005']][370:450] # before compounding
toks_comp[['text7005']][370:450] # after compounding
```

Alternatively, wrap the whitespace-separated character vector by `phrase()` to compound multi-word expressions:

```{r}
toks_comp <- tokens_compound(toks_news, 
                             pattern =  phrase(tstat_col_cap$collocation[tstat_col_cap$z > 3]))
toks_news[['text7005']][370:450] # before compounding
toks_comp[['text7005']][370:450] # after compounding
```
