---
title: Construct a tokens object
weight: 10
draft: false
---

```{r message=FALSE}
require(quanteda)
```

`tokens()` segments texts in a corpus into tokens (words or sentences) by word boundaries. 

```{r}
corp_immig <- corpus(data_char_ukimmig2010)
toks_immig <- tokens(corp_immig)
```

A corpus is passed to `tokens()` in the code above, but it works with a character string too.

```{r}
toks <- tokens(data_char_ukimmig2010)
head(toks[[1]], 50)
```

{{% notice tip %}}
`tokens()` can tokenize texts in Asian languages such as Japanese or Chinese without additional tools thanks to the **stringi** package. See an example of in [documentation website](https://quanteda.io/articles/pkgdown/examples/chinese.html).
{{% /notice %}}

By default, `tokens()` only removes separators (typically whitespaces), but you can remove punctuation and numbers.

```{r}
toks_nopunct <- tokens(data_char_ukimmig2010, remove_punct = TRUE)
head(toks_nopunct[[1]], 50)
```


{{% notice tip %}}
**stringi**'s word boundary detection splits Twitter hashtags into # and following keyword, but `tokens()` preserves these unless you set `remove_twitter = TRUE`.
{{% /notice %}}



