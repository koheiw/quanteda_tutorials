---
title: Japanese
weight: 10
draft: false
---

{{% author %}}By Kohei Watanabe{{% /author %}} 

```{r, message=FALSE}
require(quanteda)
require(quanteda.corpora)
```

## Tokenization

You can use `tokens()` or a morphological analysis tool such as [Mecab](http://taku910.github.io/mecab/) to tokenize Japanese texts. The sample corpus contains transcripts of all the speeches at Japan's Committee on Foreign Affairs and Defense of the lower house (Shugiin) from 1947 to 2017

```{r, eval=FALSE}
corp <- download("data_corpus_foreignaffairscommittee")

# use the latest 1000 speeches
txt <- tail(texts(corp), 1000)
```

```{r include=FALSE}
# This code is only for website generation
corp <- readRDS("/home/kohei/Dropbox/Public/data_corpus_foreignaffairscommittee.rds")
#corp <- readRDS("/Users/smueller/Dropbox/Corpora/data_corpus_foreignaffairscommittee.rds")
txt <- tail(texts(corp), 1000)
```

### Dictionary-based boundary detection

`tokens()` can segment Japanese texts without additional tools based on the rules defined in the [ICU library](http://site.icu-project.org/home), which is available via the **stringi** package. ICU detects boundaries of Japanese words using [a dictonary with frequency information](http://source.icu-project.org/repos/icu/icu/tags/release-58-rc/source/data/brkitr/dictionaries/).

```{r}
toks_icu <- tokens(txt)
head(toks_icu[[20]], 100)
```

### Morphological analysis

If you want to perform more accurate tokenization, you need to install a morphological analysis tool, and call it from R. [Mecab](https://en.wikipedia.org/wiki/MeCab) is one of the most popular tools for tokenizing Japanese texts, and we can use it from R with **RMecab**. The package is not available on CRAN, so you have to install the library from the [author's repository](http://rmecab.jp).

```{r include=FALSE}
# This code is only for website generation
toks_mecab <- readRDS("../data/data_tokens_japanese.rds")
```

```{r, eval=FALSE}
install.packages("RMeCab", repos = "http://rmecab.jp/R")
library(RMeCab)
toks_mecab <- txt %>% 
  lapply(function(x) unlist(RMeCab::RMeCabC(x))) %>% 
  as.tokens()
```

```{r}
head(toks_mecab[[20]], 100)
```


## Refining tokens

Even if you use a morphological analysis tool, tokenization of Japanese text is far from perfect. However, you can refine tokens by compounding sequence of the same character class using `textstat_collocations()` and `tokens_compound()`. `^[一-龠]+$` and `^[ァ-ヶー]+$` regular expressions that match tokens are compromised only of kanji and katakana, respectively. `^[０-９ァ-ヶー一-龠]+$` is for tokens that is a combination of numbers, katakana, or kanji.

```{r}
toks_icu_refi <- toks_icu
tstat_kanji <- toks_icu %>% 
    tokens_select('^[一-龠]+$', valuetype = 'regex', padding = TRUE) %>% 
    textstat_collocations(min_count = 5, tolower = FALSE)
toks_icu_refi <- tokens_compound(toks_icu_refi, tstat_kanji[tstat_kanji$z > 2],
                                 concatenator = '', join = TRUE)

tstat_kana <- toks_icu_refi %>% 
    tokens_select('^[ァ-ヶー]+$', valuetype = 'regex', padding = TRUE) %>% 
    textstat_collocations(min_count = 5, tolower = FALSE)
toks_icu_refi <- tokens_compound(toks_icu_refi, tstat_kana[tstat_kana$z > 2],
                                 concatenator = '', join = TRUE)

tstast_any <- toks_icu_refi %>% 
            tokens_select('^[０-９ァ-ヶー一-龠]+$', valuetype = 'regex', padding = TRUE) %>% 
            textstat_collocations(min_count = 5, tolower = FALSE)
toks_icu_refi <- tokens_compound(toks_icu_refi, tstast_any[tstast_any$z > 2],
                                 concatenator = '', join = TRUE)

```

```{r}
head(toks_icu_refi[[20]], 100)
```

{{% notice note %}}
You have to be careful not to compound too many tokens to reduce their semantic ambiguity, because large ngrams increase sparsity of the data and make statistical analysis more difficult.
{{% /notice %}}

## Feature selection

We did not remove punctuation marks in the initial tokenization, but we do it here by `tokens(remove_punct = TRUE)`. There is no standard list of stop words for Japanese, but we can hiragana-only tokens are usually function words that we can remove.

```{r}
toks_icu_refi <- tokens(toks_icu_refi, remove_punct = TRUE)
toks_icu_refi <- tokens_remove(toks_icu_refi, "^[ぁ-ん]+$", valuetype = "regex")
head(toks_icu_refi[[20]], 100)
```

Even after hiragana-only tokens are removed, there are many one-character tokens, many of which are very common verbs in Japanese texts. These tokens can be removed by `dfm_select(min_nchar = 2)`.

```{r}
dfmat_icu_refi <- dfm(toks_icu_refi, tolower = FALSE)
topfeatures(dfmat_icu_refi, 20)
dfmat_icu_refi <- dfm_select(dfmat_icu_refi, min_nchar = 2)
topfeatures(dfmat_icu_refi, 20)
```

{{% notice info %}}
If you want to learn more about how to analyze speeches at at Japan's Committee on Foreign Affairs and Defense of the lower house, see the [example](https://docs.quanteda.io/articles/pkgdown/examples/japanese_speech_ja.html) on the documentation site.
{{% /notice%}}
