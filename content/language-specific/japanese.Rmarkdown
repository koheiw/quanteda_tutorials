---
title: Japanese
weight: 10
draft: false
---

{{% author %}}By Kohei Watanabe{{% /author %}} 

```{r, message=FALSE}
require(quanteda)
require(quanteda.corpora)
```

## Tokenization

You can use `tokens()` or a morphological analysis tool such as [Mecab](http://taku910.github.io/mecab/) to tokenize Japanese texts. The sample corpus contains transcripts of all the speeches at Japan's Committee on Foreign Affairs and Defense of the lower house (Shugiin) from 1947 to 2017.

```{r, eval=FALSE}
corp <- download("data_corpus_foreignaffairscommittee")
txt <- tail(texts(corp), 1000)
```

```{r include=FALSE}
# This code is only for website generation
corp <- readRDS("/home/kohei/Dropbox/Public/data_corpus_foreignaffairscommittee.rds")
txt <- tail(texts(corp), 1000)
```

### Dictionary-based boundary detection

`tokens()` can segment Japanese texts without additional tools based on the rules defined in the [ICU library](http://site.icu-project.org/home), which is available via the **stringi** package. ICU detects boundaries of Japanese words using [a dictonary with frequency information](http://source.icu-project.org/repos/icu/icu/tags/release-58-rc/source/data/brkitr/dictionaries/).

```{r}
icu_toks <- tokens(txt)
head(icu_toks[[20]], 100)
```

### Morphological analysis

If you want to perform more accurate tokenization, you need to install a morphological analysis tool, and call it from R. [Mecab](https://en.wikipedia.org/wiki/MeCab) is one of the most popular tools for tokenizing Japanese texts, and we can use it from R with **RMecab**. The package is not available on CRAN, so you have to install the library from the [author's repository](http://rmecab.jp).

```{r include=FALSE}
# This code is only for website generation
mecab_toks <- readRDS("../data/data_tokens_japanese.rds")
```

```{r, eval=FALSE}
install.packages("RMeCab", repos = "http://rmecab.jp/R")
mecab_toks <- 
  txt %>% 
  lapply(function(x) unlist(RMeCab::RMeCabC(x))) %>% 
  as.tokens()
```

```{r}
head(mecab_toks[[20]], 100)
```


## Refining tokens

Even if you use a morphological analysis tool, tokenization of Japanese text is far from perfect. However, you can refine tokens by compounding sequence of the same character class using `textstat_collocations()` and `tokens_compound()`. `^[一-龠]+$` and `^[ァ-ヶー]+$` regular expressions that match tokens are compromised only of kanji and katakana, respectively. `^[０-９ァ-ヶー一-龠]+$` is for tokens that is a combination of numbers, katakana, or kanji.

```{r}
refi_icu_toks <- icu_toks
seqs_kanji <- icu_toks %>% 
              tokens_select('^[一-龠]+$', valuetype = 'regex', padding = TRUE) %>% 
              textstat_collocations(min_count = 5, tolower = FALSE)
refi_icu_toks <- tokens_compound(refi_icu_toks, seqs_kanji[seqs_kanji$z > 2], concatenator = '', join = TRUE)

seqs_kana <- refi_icu_toks %>% 
             tokens_select('^[ァ-ヶー]+$', valuetype = 'regex', padding = TRUE) %>% 
             textstat_collocations(min_count = 5, tolower = FALSE)
refi_icu_toks <- tokens_compound(refi_icu_toks, seqs_kana[seqs_kana$z > 2], concatenator = '', join = TRUE)

seqs_any <- refi_icu_toks %>% 
            tokens_select('^[０-９ァ-ヶー一-龠]+$', valuetype = 'regex', padding = TRUE) %>% 
            textstat_collocations(min_count = 5, tolower = FALSE)
refi_icu_toks <- tokens_compound(refi_icu_toks, seqs_any[seqs_any$z > 2], concatenator = '', join = TRUE)

```

```{r}
head(refi_icu_toks[[20]], 100)
```

{{% notice note %}}
You have to be careful not to compound too many tokens to reduce their semantic ambiguity, because large ngrams increase sparsity of the data and make statistical analysis more difficult.
{{% /notice %}}

## Feature selection

We did not remove punctuation marks in the initial tokenization, but we do it here by `tokens(remove_punct = TRUE)`. There is no standard list of stop words for Japanese, but we can hiragana-only tokens are usually function words that we can remove.

```{r}
refi_icu_toks <- tokens(refi_icu_toks, remove_punct = TRUE)
refi_icu_toks <- tokens_remove(refi_icu_toks, "^[ぁ-ん]+$", valuetype = "regex")
head(refi_icu_toks[[20]], 100)
```

Even after hiragana-only tokens are removed, there are many one-character tokens, many of which are very common verbs in Japanese texts. These tokens can be removed by `dfm_select(min_nchar = 2)`.

```{r}
refi_icu_dfm <- dfm(refi_icu_toks, tolower = FALSE)
topfeatures(refi_icu_dfm, 20)
refi_icu_dfm <- dfm_select(refi_icu_dfm, min_nchar = 2)
topfeatures(refi_icu_dfm, 20)
```

{{% notice info %}}
If you want to learn more about how to analyze speeches at at Japan's Committee on Foreign Affairs and Defense of the lower house, see the [ example](https://docs.quanteda.io/articles/pkgdown/examples/japanese_speech_ja.html) on the documentation site.
{{% /notice%}}
