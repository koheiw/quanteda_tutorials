---
title: Statistical analysis
weight: 10
chapter: false
draft: true
---

```{r}
require(quanteda)
load("../data/guardianSample.RData")
```

You can also download [Guardian corpus](https://github.com/kbenoit/QTAUR-halfday/blob/master/data/guardianSample.RData) and save it in you computer.

This corpus contains 6,000 Guardian news articles from 2012 to 2016.

```{r}
ndoc(guardianSample)
range(docvars(guardianSample, 'date'))
```

Create new document variables and tokenize texts:

```{r}
docvars(guardianSample, 'month') <- format(docvars(guardianSample, 'date'), '%Y-%m')
docvars(guardianSample, 'year') <- format(docvars(guardianSample, 'date'), '%Y')

toks <- tokens(guardianSample)
toks <- tokens_select(toks, stopwords('english'), select = 'remove', padding = TRUE) 
toks <- tokens_select(toks, '^[\\p{P}\\p{S}]$', select = 'remove', valuetype = 'regex', padding = TRUE)
```

## Collocation analysis

By collocation analysis, we can identify multi-word expressions that are very frequent in newspapers articles. One of the most common type of multi-word expressions is proper names, which can be identified simply based on capitalization in English texts.

```{r}
cap_toks <- tokens_select(toks, '^[A-Z]', valuetype = 'regex', case_insensitive = FALSE, padding = TRUE)
cap_col <- textstat_collocations(cap_toks, min_count = 5)
head(cap_col, 20)
```

### Compound multi-word expressions

The result of collocation analysis is not only interesting but useful: you can be use it to compound tokens. Compounding makes tokens less ambiguous and significantly improves quality of statistical analysis in the downstream. We will only compound strongly associated (p<0.998) multi-word expressions here by sub-setting `cap_col$collocation`.

```{r}
comp_toks <- tokens_compound(toks, phrase(cap_col$collocation[cap_col$z > 3]))
toks[['text7005']][370:450] # before compounding
comp_toks[['text7005']][370:450] # after compounding
```

## Relative frequency analysis

Keyness is a statistical measure originally implemented in [WordSmith](http://www.lexically.net/wordsmith/) to discover frequent words in target documents. This statistic is essentially a signed chi-square, where words more frequent than expected are given positive sign. 

### Frequent words in 2016

```{r}
news_dfm <- dfm(comp_toks)
key <- textstat_keyness(news_dfm, docvars(news_dfm, 'year') == 2016) 
head(key, 20)
textplot_keyness(key)
```

### Frequent words on Brexit

We can also find words associated with target words using the `window` argument of `tokens_select()`. 

```{r}
brexit_toks <- tokens_keep(comp_toks, 'brexit', window = 10) # equivalent to tokens_select(selection = 'keep')
not_brexit_toks <- tokens_remove(comp_toks, 'brexit', window = 10) # equivalent to tokens_select(selection = 'remove')
print(brexit_toks[['text173244']])

dfm_brexti <- dfm(brexit_toks)
not_brexit_dfm <- dfm(not_brexit_toks)

key_brexit <- textstat_keyness(rbind(dfm_brexti, not_brexit_dfm), seq_len(ndoc(dfm_brexti)))
key_brexit <- key_brexit[key_brexit$n_target > 10,]
head(key_brexit, 20)
textplot_keyness(key_brexit[-1,])
```

### Frequent words on Trump

Targeted frequency analysis might look complex, but can be done in three lines.

```{r}
trump_dfm <- dfm(tokens_keep(comp_toks, c('donald_trump', 'trump'), window = 10))
not_trump_dfm <- dfm(tokens_remove(comp_toks, c('donald_trump', 'trump'), window = 10))
key_trump <- textstat_keyness(rbind(trump_dfm, not_trump_dfm), seq_len(ndoc(trump_dfm)))

head(key_trump[key_trump$n_target > 10,], 20)
textplot_keyness(key_trump[c(-1, -2, -3),])
```

##  Dictionary-based analysis

Dictionary-based analysis is one of the most popular quantitative text analysis methods. You can define keys and values of a dictionary using `dictionary()` (it also import the Wordstat, LIWC, Yoshicoder, Lexicoder and YAML formats through `file` argument).

```{r}
dict <- dictionary(list(positive = c('good', 'nice', 'excellent'), 
                        negative = c('bad', 'nasty', 'poor')))
print(dict)
```

You can use `tokens_lookup()` or `dfm_looup()` to count dictionary values. **quanteda** contains Lexicoder Sentiment Dictionary created by Young and Soroka, so you can perfrom sentiment analysis of English texts right away.

```{r}
lengths(data_dictionary_LSD2015)

lsd_toks <- tokens_lookup(toks, data_dictionary_LSD2015)
head(lsd_toks, 2)

dfm_lsd <- dfm(lsd_toks)
head(dfm_lsd, 2)
```

### Aggregate frequency counts

We can aggregate frequency count using the `groups` argument of `dfm()`.

```{r}
lsd_year_dfm <- dfm(lsd_toks, groups = docvars(lsd_toks, 'year'))
head(lsd_year_dfm)
```

### Targeted analysis

You can use `tokens_select()` with `window` argument to perform more targeted sentiment analysis.

```{r}
lsd_brexit_dfm <- dfm(brexit_toks, dictionary = data_dictionary_LSD2015[1:2], 
                      groups = docvars(brexit_toks, 'month'))
lsd_brexit_dfm <- tail(lsd_brexit_dfm[order(docnames(lsd_brexit_dfm))], 24) # last 24 month only

matplot(lsd_brexit_dfm[,c(1:2)], type = 'l', xaxt = 'n', lty = 1, 
        main = 'Sentiment on Brexit', ylab = 'Frequency')
grid()
axis(1, 1:24, docnames(lsd_brexit_dfm))
legend('topleft', col = 1:2, legend = c('Negative', 'Positive'), lty = 1)
```

```{r}
plot(lsd_brexit_dfm[,2] - lsd_brexit_dfm[,1], type = 'b', main = 'Sentiment on Brexit (difference)',
     ylab = 'Frequency', xlab = '', xaxt = 'n')
axis(1, 1:24, docnames(lsd_brexit_dfm))
grid()
abline(h = 0, lty = 2)
```


```{r}
lsd_trump_dfm <- dfm(tokens_keep(comp_toks, c('donald_trump', 'trump'), window = 10), 
                     dictionary = data_dictionary_LSD2015[1:2], groups = docvars(comp_toks, 'month'))
lsd_trump_dfm <- tail(lsd_trump_dfm[order(docnames(lsd_trump_dfm))], 24) # last 24 month only

matplot(lsd_trump_dfm, type = 'l', xaxt = 'n', lty = 1, 
        main = 'Sentiment on Trump', ylab = 'Frequency')
grid()
axis(1, 1:24, docnames(lsd_trump_dfm))
legend('topleft', col = 1:2, legend = c('Negative', 'Positive'), lty = 1)
```

```{r}
plot(lsd_trump_dfm[,2] - lsd_trump_dfm[,1], type = 'b', main = 'Sentiment on Trump (difference)',
     ylab = 'Frequency', xlab = '', xaxt = 'n')
axis(1, 1:24, docnames(lsd_trump_dfm))
grid()
abline(h = 0, lty = 2)
```